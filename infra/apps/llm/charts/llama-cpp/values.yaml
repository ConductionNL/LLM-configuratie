# values.yaml - LLM chart (zorgt dat pods alleen op role=llm gepland worden)
replicaCount: 1

strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 0
    maxSurge: 1

progressDeadlineSeconds: 1800

image:
  repository: ghcr.io/ggml-org/llama.cpp
  tag: server
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

service:
  type: ClusterIP
  port: 8080

podAnnotations: {}

# --- Scheduling / isolation (belangrijk voor exclusieve llm nodes) ---
nodeSelector:
  role: llm

tolerations:
  - key: "llm-only"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
                - llm
        topologyKey: "kubernetes.io/hostname"
# ---------------------------------------------------------------

# Resources: laat leeg of stel realistische values in
resources: {}
  # Voorbeeld suggestie (activeer indien gewenst):
  # requests:
  #   cpu: "2000m"
  #   memory: "4Gi"
  # limits:
  #   cpu: "4000m"
  #   memory: "8Gi"

# Extra environment variables for the server container
extraEnv: []
# - name: SOME_ENV
#   value: some-value

# Additional arguments appended to the llama.cpp server binary
extraArgs: []

# Shared models volume
volume:
  models:
    mountPath: /models
    # sizeLimit: "" # optional (emptyDir)

huggingFace:
  secretName: hf-token
  tokenKey: HUGGING_FACE_HUB_TOKEN

model:
  enabled: true
  # Either provide repoId+filename or an explicit downloadUrl
  repoId: ""
  filename: ""
  downloadUrl: ""

initDownloader:
  enabled: true
  image:
    repository: curlimages/curl
    tag: "8.10.1"
    pullPolicy: IfNotPresent
